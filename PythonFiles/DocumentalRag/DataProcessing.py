import fitz  # PyMuPDF
import re
from langchain.text_splitter import TokenTextSplitter
from sentence_transformers import SentenceTransformer
import FlexibleChunker as fc
from langchain_community.utilities import SQLDatabase
import logging
import textwrap
from sqlalchemy import text

class DocumentProcessor:
    def __init__(self, model_name="efederici/sentence-BERTino"):
        self.model = SentenceTransformer(model_name)
        self.chunker = fc.FlexibleChunker(768)
        self.document_path=r"D:\RagApplications\Documenti\ManualeTETRAS_modificato2.pdf"


    def extract_subtitles_and_text(self, start_page=0):
        document = fitz.open(self.document_path)
        subtitles_dict = {}
        pattern = re.compile(r'\d+\.\d+')
        current_subtitle = None
        current_subtitle_name = None
        current_text = ""

        for page_num in range(start_page, len(document)):
            page = document.load_page(page_num)
            text = page.get_text("text")
            lines = text.split('\n')

            for i, line in enumerate(lines):
                if pattern.match(line.strip()):
                    if current_subtitle:
                        subtitles_dict[current_subtitle] = (current_subtitle_name.replace(".", ""), re.sub(r'\d+', '', current_text).strip())
                    current_subtitle = line.strip().split()[0]
                    if len(line.strip().split()) > 1:
                        current_subtitle_name = ' '.join(line.strip().split()[1:])
                    else:
                        current_subtitle_name = lines[i + 1].strip() if i + 1 < len(lines) else ""
                    current_text = ""
                elif line.strip() != current_subtitle_name:
                    current_text += " " + line.strip()

            if current_subtitle:
                subtitles_dict[current_subtitle] = (current_subtitle_name.replace(".", ""), re.sub(r'\d+', '', current_text).strip())
                
        subtitles_dict.pop("4.2", None)
        return subtitles_dict


    def generate_chunked_dictionaries(self, data_dict, chunk_size=768, overlap_percentage=[0.1, 0.2, 0.3]):
        
        dictList = []
        for overlap in overlap_percentage:
            overlap = int(chunk_size * overlap)
            chunked_subtitles_dict = data_dict.copy()
            for subtitle, text in data_dict.items():
                if len(text[1]) <= int(chunk_size*3.5):
                    chunked_subtitles_dict[subtitle] = [text[1]]
                else:
                    chunked_subtitles_dict[subtitle] = self.chunker.chunk_with_overlap(text[1], overlap_size=overlap)
            dictList.append(chunked_subtitles_dict)
        return dictList
        

    def generate_chunked_dictionary_ii(self, data_dict):
        custom_chunker = fc.FlexibleChunker(128)
        chunked_subtitles_dict = data_dict.copy()
        for subtitle, text in data_dict.items():
            document = self.chunker.chunk_text(text[1])
            chunked_subtitles_dict[subtitle] = [(large_chunk, custom_chunker.chunk_with_overlap(large_chunk, 28)) for large_chunk in document]
        return chunked_subtitles_dict
    
    def generate_question_dictionary(self, data_dict, chunk_size : int = 350):

        chunker = fc.FlexibleChunker(target_chunk_size=chunk_size)

        dict_for_questions = {}

        for subtitle, text in data_dict.items():
            dict_for_questions[subtitle] = chunker.chunk_text(text[1])

        for subtitle, text in dict_for_questions.items():
            if chunker.count_tokens(text[-1]) < 100 and len(text) > 1:
                text[-2] += " " + text[-1]
                text.pop(-1)
        return dict_for_questions


    def get_connection_to_sql_database(self, host="localhost", data_base_name="database", username="postgres", password="postgres", port="5432"):
        g_uri = f"postgresql+psycopg2://{username}:{password}@{host}:{port}/{data_base_name}"
        return SQLDatabase.from_uri(g_uri)

    def create_tables(self, db):
        db.run("CREATE EXTENSION IF NOT EXISTS vector_scale CASCADE;")
        db.run("""
        DO $$
        BEGIN
            IF NOT EXISTS (SELECT 1 FROM pg_tables WHERE tablename = 'documents') THEN
                CREATE TABLE documents (
                    id BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
                    content TEXT NOT NULL,
                    index TEXT NOT NULL,
                    embedding VECTOR(768) NOT NULL
                );
                CREATE INDEX document_embedding_idx ON documents
                USING disk_ann (embedding);
            END IF;
        END $$;
        """)

    def create_tables_ii(self, db):
        db.run("""
        CREATE EXTENSION IF NOT EXISTS vector_scale CASCADE;
        CREATE TABLE document (
        chunk_id INT PRIMARY KEY,
        content TEXT,
        index TEXT
        );

        CREATE TABLE small_chunks (
        small_chunk_id SERIAL PRIMARY KEY,
        large_chunk_id INTEGER REFERENCES document(chunk_id),
        embedding vector(128)
        );

        CREATE INDEX document_embedding_idx ON small_chunks
        USING diskann (embedding);
        """)
        
    def insert_data(self, db, data_dict):
        engine = db._engine

        for index, chunks in data_dict.items():
            embeddings = self.model.encode(chunks)
            for chunk, embedding in zip(chunks, embeddings):
                logging.info(f"Inserting chunk with index: {index} and content: {chunk[:10]}...")
                
                query = text("""
                INSERT INTO documents (content, index, embedding) 
                VALUES (:content, :index, :embedding)
                """)
                with engine.connect() as conn:
                    conn.execute(
                        query,
                        {
                            "content": chunk,
                            "index": index,
                            "embedding": embedding.tolist()
                        }
                    )
                    conn.commit()

    def insert_data_ii(self, db, chunked_subtitles_dict):
        """
        Insert hierarchical chunks (large chunks and their associated small chunks) into the database
        
        Args:
            db (SQLDatabase): LangChain SQLDatabase instance
            chunked_subtitles_dict (dict): Dictionary mapping indices to tuples of (large_chunk, small_chunks)
        """
        # Get the underlying SQLAlchemy engine
        engine = db._engine
        
        for index, chunks in chunked_subtitles_dict.items():
            for large_chunk, small_chunks in chunks:
                # Use a transaction to ensure data consistency
                with engine.begin() as conn:
                    # Insert large chunk and get its ID                    
                    insert_large_query = text("""
                        INSERT INTO document (content, index) 
                        VALUES (:content, :index) 
                        RETURNING chunk_id
                    """)
                    
                    result = conn.execute(
                        insert_large_query,
                        {"content": large_chunk, "index": index}
                    )
                    large_chunk_id = result.scalar()  # Get the returned chunk_id
                    
                    # Insert all associated small chunks
                    insert_small_query = text("""
                        INSERT INTO small_chunks (large_chunk_id, embedding) 
                        VALUES (:large_chunk_id, :embedding)
                    """)
                    
                    for small_chunk in small_chunks:
                        embedding = self.model.encode(small_chunk)
                        conn.execute(
                            insert_small_query,
                            {
                                "large_chunk_id": large_chunk_id,
                                "embedding": embedding.tolist()
                            }
                        )


    def process_data(self):
        
        logging.basicConfig(level=logging.INFO)
        
        logging.info("Starting data processing...")

        subtitles_dict = self.extract_subtitles_and_text()
        logging.info("Extracted subtitles and text.")

        question_dict = self.generate_question_dictionary(subtitles_dict, chunk_size=350)
        logging.info("Generated question dictionary.")
        

        dict_list = self.generate_chunked_dictionaries(subtitles_dict)
        logging.info("Generated chunked dictionaries.")
        """""
        for dict in dict_list:
            for subtitle, chunks in dict.items():
                print(f"Subtitle: {subtitle}")
                for chunk in chunks:
                    print(textwrap.fill(chunk, width=80))
                    print("\n")
        """

        hybrid_dict = self.generate_chunked_dictionary_ii(subtitles_dict)
        logging.info("Generated hybrid chunked dictionary.")
        """
        for subtitle, chunks in dict.items():
                print(f"Subtitle: {subtitle}")
                for chunk in chunks:
                    print(textwrap.fill(chunk, width=80))
                    print("\n")"""
        
        
        db1 = self.get_connection_to_sql_database(host="localhost", data_base_name="DBTestChunking1", username="postgres", password="578366")
        db2 = self.get_connection_to_sql_database(host="localhost", data_base_name="DBTestChunking2", username="postgres", password="578366")
        db3 = self.get_connection_to_sql_database(host="localhost", data_base_name="DBTestChunking3", username="postgres", password="578366")
        db_hybrid = self.get_connection_to_sql_database(host="localhost", data_base_name="DBTestHybrid", username="postgres", password="578366")
        logging.info("Connected to SQL databases.")

        dbs_chunking = [db1, db2, db3]
        response = input("Do you want to add the data to the overlap dbs? (yes/no): ").strip().lower()
        if response == 'yes':
            for db, data_dict in zip(dbs_chunking, dict_list):
                self.insert_data(db, data_dict)
                logging.info(f"Inserted data into database: {db}")
        
        response = input("Do you want to add the data to the hybrid db? (yes/no): ").strip().lower()
        if response == 'yes':
            self.insert_data_ii(db_hybrid, hybrid_dict)
            logging.info("Inserted hybrid data into database.")

        logging.info("Data processing completed.")
        
        return question_dict, dbs_chunking, db_hybrid